---
title       : PortfolioAnalytics
subtitle    : R/Finance 2015
author      : Ross Bennett
date        : May 29, 2015
framework   : io2012 # {io2012, html5slides, shower, dzslides, ...}
ext_widgets : {rCharts: libraries/nvd3}
widgets     : mathjax
mode        : selfcontained
---

```{r, echo=FALSE, message=FALSE}
library(PortfolioAnalytics)
require(methods)
```



## Overview
* Discuss Portfolio Optimization
* Introduce PortfolioAnalytics
* Demonstrate PortfolioAnalytics with Examples

<!--
* Discuss Portfolio Optimization
    * Background and challenges of portfolio theory
* Introduce PortfolioAnalytics
    * What PortfolioAnalytics does and the problems it solves
* Demonstrate PortfolioAnalytics with Examples
    * Brief overview of the examples I will be giving
-->

---

## Modern Portfolio Theory
"Modern" Portfolio Theory (MPT) was introduced by Harry Markowitz in 1952.

In general, MPT states that an investor's objective is to maximize portfolio expected return for a given amount of risk.

General Objectives

* Maximize a measure of gain per unit measure of risk
* Minimize a measure of risk

How do we define risk? What about more complex objectives and constraints?

<!--
Several approaches follow the Markowitz approach using mean return as a measure of gain and standard deviation of returns as a measure of risk. This is an academic approach. 
-->

---

## Portfolio Optimization Objectives
* Minimize Risk
    * Volatility
    * Tail Loss (VaR, ES)
    * Other Downside Risk Measure
* Maximize Risk Adjusted Return
    * Sharpe Ratio, Modified Sharpe Ratio
    * Several Others
* Risk Budgets
    * Equal Component Contribution to Risk (i.e. Risk Parity)
    * Limits on Component Contribution
* Maximize a Utility Function
    * Quadratic, CRRA, etc.

<!--
* Expand on pros/cons of closed-form solvers vs. global solvers and what objectives can be solved.
* The challenge here is knowing what solver to use and the capabilities/limits of the chosen solver. 
* Some of these problems can be formulated as a quadratic or linear programming problem. Constructing the constraint matrix and objective function matrix or vector is not trivial. Limited to the quality of LP and QP solvers available for R. 
-->

---

## PortfolioAnalytics Overview
PortfolioAnalytics is an R package designed to provide numerical solutions and visualizations for portfolio optimization problems with complex constraints and objectives.

* Support for multiple constraint and objective types
* An objective function can be any valid R function
* Modular constraints and objectives
* Support for user defined moment functions
* Visualizations
* Solver agnostic
* Support for parallel computing

<!---
The key points to make here are:
* Flexibility
    * The multiple types and modularity of constraints and objectives allows us to add, remove, combine, etc. multiple constraint and objective types very easily.
    * Define an objective as any valid R function
    * Define a function to compute the moments (sample, robust, shrinkage, factor model, GARCH model, etc.)
    * Estimation error is a significant concern with optimization. Having the ability to test different models with different parameters is critical.
* PortfolioAnalytics comes "out of the box" with several constraint types.
* Visualization helps to build intuition about the problem and understand the feasible space of portfolios
* Periodic rebalancing and analyzing out of sample performance will help refine objectives and constraints
* Framework for evaluating portfolios with different sets of objectives and portfolios through time
-->

---

## New in PortfolioAnalytics

* Pushed to CRAN
* Regime Switching Framework
* Multilayer Optimization
* Rank Based Optimization
* Factor Model Moment Estimates
* Improved Random Portfolios Algorithm
* More demos, vignettes, and documentation

<!--
Highlight a few things about each point
* Pushed to CRAN
  * 2015-04-19
* Regime Switching Framework
  * very general framework to define 'n' portfolios for 'n' regimes
  * useful for out of sample backtesting
* Multilayer Optimization
  * more on this in next slide
* Rank Based Optimization
  * examples
* Factor Model Moment Estimates
  * statistical factor model
  * compute higher order moments based on the work of kris boudt
* Improved Random Portfolios Algorithm
  * support more constraints by construction
  * more efficient for group constraints
* More demos, vignettes, and documentation
  * added vignette for custom moments and objectives
  * demos for each new feature added last summer during GSoC 2014
-->

---

## Multilayer Optimization
![](mult_portf.png)


---

## Support Multiple Solvers
Linear and Quadratic Programming Solvers

* R Optimization Infrastructure (ROI)
    * GLPK (Rglpk)
    * Symphony (Rsymphony)
    * Quadprog (quadprog)

Global (stochastic or continuous solvers)

* Random Portfolios
* Differential Evolution (DEoptim)
* Particle Swarm Optimization (pso)
* Generalized Simulated Annealing (GenSA)

<!---
Brief explanation of each solver and what optimization problems are supported
-->

---

## Random Portfolios
PortfolioAnalytics has three methods to generate random portfolios.

1. The **sample** method to generate random portfolios is based on an idea by Pat Burns.
2. The **simplex** method to generate random portfolios is based on a paper by W. T. Shaw.
3. The **grid** method to generate random portfolios is based on the `gridSearch` function in the NMOF package.

<!--
* Random portfolios allow one to generate an arbitray number of portfolios based on given constraints. Will cover the edges as well as evenly cover the interior of the feasible space. Allows for massively parallel execution.

* The sample method to generate random portfolios is based on an idea by Patrick Burns. This is the most flexible method, but also the slowest, and can generate portfolios to satisfy leverage, box, group, and position limit constraints.

* The simplex method to generate random portfolios is based on a paper by W. T. Shaw. The simplex method is useful to generate random portfolios with the full investment constraint, where the sum of the weights is equal to 1, and min box constraints. Values for min_sum and max_sum of the leverage constraint will be ignored, the sum of weights will equal 1. All other constraints such as the box constraint max, group and position limit constraints will be handled by elimination. If the constraints are very restrictive, this may result in very few feasible portfolios remaining. Another key point to note is that the solution may not be along the vertexes depending on the objective. For example, a risk budget objective will likely place the portfolio somewhere on the interior.

* The grid method to generate random portfolios is based on the gridSearch function in NMOF package. The grid search method only satisfies the min and max box constraints. The min_sum and max_sum leverage constraint will likely be violated and the weights in the random portfolios should be normalized. Normalization may cause the box constraints to be violated and will be penalized in constrained_objective.
-->

---

## Comparison of Random Portfolio Methods (Interactive!)

RP

<!--
```{r, results = 'asis', comment = NA, message = F, echo = F}
load("figures/rp_viz.rda")
rp_viz$show('inline')
```

The feasible space is computed using the EDHEC data for a long only portfolio with a search size of 2000.
-->

---

## Random Portfolios: Simplex Method

RP Simplex

<!--
FEV (Face-Edge-Vertex bias values control how concentrated a portfolio is. This can clearly be seen in the plot. As FEV approaches infinity, the portfolio weight will be concentrated on a single asset. PortfolioAnalytics allows you to specify a vector of fev values for comprehensive coverage of the feasible space. 
-->

---

## Workflow: Specify Portfolio
```{r}
args(portfolio.spec)
```

Initializes the portfolio object that holds portfolio level data, constraints, and objectives

<!--
The portfolio object is an S3 object that holds portfolio-level data, constraints, and objectives. The portfolio-level data includes asset names and initial weights, labels to categorize assets, and a sequence of weights for random portfolios. The main argument is assets which can be a character vector (most common use), named numeric vector, or scalar value specifying number of assets.
-->

---

## Workflow: Add Constraints
```{r}
args(add.constraint)
```

Supported Constraint Types

* Sum of Weights
* Box
* Group
* Factor Exposure
* Position Limit
* and many more

<!--
This adds a constraint object to the portfolio object. Constraints are added to the portfolio object with the add.constraint function. Each constraint added is a separate object and stored in the constraints slot in the portfolio object. In this way, the constraints are modular and one can easily add, remove, or modify the constraints in the portfolio object. Main argument is the type, arguments to the constraint constructor are then passed through the dots (...).
-->

---

## Workflow: Add Objectives
```{r}
args(add.objective)
```

Supported Objective types

* Return
* Risk
* Risk Budget
* Weight Concentration

<!--
Objectives are added to the portfolio object with the add.objective function. Each objective added is a separate object and stored in the objectives slot in the portfolio object. In this way, the objectives are modular and one can easily add, remove, or modify the objective objects. The name argument must be a valid R function. Several functions are available in the PerformanceAnalytics package, but custom user defined functions can be used as objective functions.
-->

---

## Workflow: Run Optimization
```{r}
args(optimize.portfolio)
args(optimize.portfolio.rebalancing)
```

<!--
* Notice the similarity between these two functions. You only have to specify a few additional arguments for the backtesting.

* optimize.portfolio: Main arguments for a single period optimization are the returns (R), portfolio, and optimize_method. We take the portfolio object and parse the constraints and objectives according to the optimization method.

* optimize.portfolio.rebalancing: Supports periodic rebalancing (backtesting) to examine out of sample performance. Helps refine constraints and objectives by analyzing out or sample performance. Essentially a wrapper around optimize.portfolio that handles the time interface.
-->

---

## Workflow: Analyze Results

Visualization | Data Extraction
------------- | ----------
plot | extractObjectiveMeasures
chart.Concentration | extractStats
chart.EfficientFrontier | extractWeights
chart.RiskReward | print
chart.RiskBudget | summary
chart.Weights | 

<!--
Brief explanation of each function.
-->


---

## Portfolio Optimization

![](opt_fig.png)


<!--

High level portfolio optimization framework

Inputs
  * Assets
  * Constraints
  * Objectives
  * Moments of asset returns

The assets, constraints, and objectives are defined by the portfolio manager.
In general, these are fixed and there is no estimation or uncertainty. However,
the moments of the asset returns must be estimated. The objectives defined in
the portfolio optimization problem determine which moments and comoments must
be estimated. Moments of the asset returns are key inputs to the optimization.

Beware! Optimizers are error maximizers
Bad Estimates  Bad Results
Better Estimates  Better Results
GIGO (Garbage In Garbage Out)

Mean - Variance
  * expected returns
  * covariance matrix

Minimum Variance
  * covariance matrix

Mean - Expected Shortfall (mean - ES)
  * expected returns vector
  * covariance matrix
  * coskewness matrix
  * cokurtosis matrix
  
If the returns are normally distributed, one can use the analytical formula for
ES which only requires estimates of the first and second moments.

The modified ES (based on Cornish-Fisher expansions) has been shown to deliver
accurate estimates for portfolios with nonnormal returns

For modified ES, one must estimate of the first four moments of the asset 
returns.

Minimum Expected Shortfall
  * expected returns vector
  * covariance matrix
  * coskewness matrix
  * cokurtosis matrix

Same comments as above apply here. The moments to estimate depend on the choice
of the risk measure, e.g. ES vs. modified ES.

Expected Utility
Here the moments to estimate are highly dependent on the choice of utility
function. 

Quadratic Utility
  * expected returns vector
  * covariance matrix

Fourth order expansion of the Constant Relative Risk Aversion (CRRA) Utility Function
Martellini and Ziemann (2010) and Boudt et al (2014)
  * expected returns vector (assume zero mean and omit)
  * covariance matrix
  * coskewness matrix
  * cokurtosis matrix

-->

---

## Estimating Moments

Ledoit and Wolf (2003):

> "The central message of this paper is that nobody should be using the sample
> covariance matrix for the purpose of portfolio optimization."


* Sample
* Shrinkage Estimators
* Factor Model
* Expressing Views


<!--
From Ledoit and Wolf (2003), "Honey, I Shrunk the Sample Covariance Matrix"
The central message of this paper is that nobody should be using the sample 
covariance matrix for the purpose of portfolio optimization.

Estimating moments using shrinkage estimators, factor models, views are methods
to address the disadvantages of using sample estimates. I am not making a claim
that one method is better than another. The method chosen depends on one's own
situation and information/data available.

Increase risk of estimation error as dimension of assets and parameters to
estimate increase

Sample Estimates Disadvantages
  * Estimation error and the curse of dimensionality
  * In the Mean - Variance framework, small changes in expected returns can 
  lead to extreme portfolios (large long/short positions) in the unconstrained
  case and concentrated (large positions in few assets) portfolios wth long
  only constraint.
  * Note that adding constraints have of the effect of lessening the impact of 
  estimation error. TODO: reference? I remember Doug stating this

The estimation of a covariance matrix is unstable unless the number of
historical observations T is greater than the number of assets N. 
10 years of data
daily: 2500
weekly: 520
monthly: 120

One has the choice to estimate moments with historical or simulated data.

Historical Data
  * do you have enough data?
  * missing assets
  * is it clean?

Simulated data
  * Model risk
  * How well does the model describe the data?

* Shrinkage Estimators
  * Ledoit-Wolf
  * James-Stein
* Factor Model
  * Fundamental
  * Statistical
  * Boudt et al (2014) use factor model to estimate higher order comoments
* Expressing Views
  * Black - Litterman
  * Almgren and Chriss, Portfolios from Sorts
  * Meucci, Fully Flexible Views Framework
* Other
  * Resampling
  * ?

-->

---

## Meucci Fully Flexible Views

* Reference Model
$$ X \sim f_X $$

* Views
$$ V \equiv g (X) \sim f_V $$

* Express View on Ranking
$$ m \{ V_1 \} \geq  m \{ V_2 \} \geq ... \geq  m \{ V_K \} $$

* Posterior
$$ \tilde{f}_x \equiv \underset{f \in V}{\text{ argmin    }} { entropy(f, f_x) } $$

<!--

Key difference between BL and FFV
The Black-Litterman approach allows the portfolio manager to express views
on the expected returns vector. The model quantifies views and uncertainty
of views.

The Meucci FFV framework is more general and allows one to express views on the
market. 

From Meucci paper
http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1213325

To obtain the posterior, we interpret the views as statements that distort the
prior distribution, in such a way that the least possible amount of spurious
structure is imposed. The natural index for the structure of a distribution is
its entropy. Therefore we define the posterior distribution as the one that
minimizes the entropy relative to the prior. Then by opinion pooling we assign
different confidence levels to different views and users.

Reference Model
We assume the existence of a risk model, i.e. a model for the joint
distribution of the risk factors, as represented by its probability density
function (pdf)
X \dist f\_X

Views
In the most general case, the user expresses views on generic functions of the
market g1 (X) , . . . , gK (X). These functions constitute a K -dimensional
random variable whose joint distribution is implied by the reference model
V = g (X) \dist f\_V

views in general are statements on only select features of the distribution of V
The generalized BL views are not necessarily expressed as equality constraint:
EP can process views expressed as inequalities. In particular, EP can process
ordering information, frequent in stock and bond management:

m { V\_1 } >=  m { V\_2 } >= ... >= m { V\_K }

The posterior
The posterior distribution should satisfy the views without adding additional
structure and should be as close as possible to the reference model

The posterior market distribution is the one that minimizes the relative
entropy

perform entropy minimization to obtain posterior

perform entropy minimization between a distribution f and the reference model f_x
\tilde{f}\_x = argmin{ entropy(f, f\_x) } f \in V stands for all the 
distributions consistent with the views statement

Confidence
Opinion Pooling
\tilde{f}\_x^c = (1 - c) * f\_x + c * \tilde{f}\_x
pooling parameter c in [0,1] represents the confidence levels in the views
-->

---

## Almgren-Chriss Portfolios from Sorts

* Define $ S_1, ... , S_n $ as the investment universe of $ n $ assets

* Defining Sorts
  * Single complete sort
  $$ r_1 \geq r_2 \geq ... \geq r_n $$
  * Sector based sort
  * Deciles and Other Divisions
  * Single complete sort with longs and shorts
  * others

* Centroid vector, $ c $, is defined as the center of mass of the set $ Q $.
  * where $ Q $ is the space of consistent expected returns


<!--

The Black-Litterman approach allows the portfolio manager to express views
on the expected returns vector. The model quantifies views and uncertainty
of views.

Portfolios from Sorts is a method for portfolio optimization based on replacing
expected returns with information about the order of the expected returns.
For a single complete sort we assume that each expected return direction is
equally likely: there is no bias toward some directions over others. The only
information in the model is the sort itself.

From the paper
We shall write S1, . . . , Sn for the available investment universe of n stocks.
In its most general sense, a portfolio sort is a set of inequality relationships
between the expected returns of these assets. The simplest and most common
example is a single complete sort which orders all the assets of the portfolio
by expected return from greatest to least.

r\_1 >= r\_2 >= ... >= r\_n

A sort is a set of beliefs about the first moments of the joint distribution of
returns

If we have a portfolio of stocks S1,...,Sn ordered so that r1 >= ··· >= rn then
we are positing two things. First, the obvious, that the expected returns of the
stocks, or more precisely, the joint distribution of the stocks, respect the
ordering. Second, this information is the only information we have about the
expected returns.

the information in an expected return vector relevant for optimization is
contained completely in its vector direction, not in its magnitude.

centroid vector
analytical approximation for a single complete sort
monte carlo estimate for other estimates
  * sector
  * sign
  * buckets

-->

---

## Example
```{r eval=FALSE}
# Load package and data.
library(PortfolioAnalytics)
data(edhec)
R <- edhec[,1:4]
funds <- colnames(R)

# Construct initial portfolio with basic constraints.
init.portf <- portfolio.spec(assets=funds)
init.portf <- add.constraint(portfolio=init.portf, type="weight_sum", 
                             min_sum=0.99, max_sum=1.01)
init.portf <- add.constraint(portfolio=init.portf, type="box",
                             min=0.05, max=0.5)
init.portf <- add.objective(portfolio=init.portf, type="risk", name="StdDev")
init.portf <- add.objective(portfolio=init.portf, type="return", name="mean")

# Generate random portfolios for use in the optimization.
rp <- random_portfolios(init.portf, 5000)
```

---

## Example
```{r eval=FALSE}
# Here we express views on the relative rank of the asset returns
# E{ R[,2] < R[,3] < R[,1] < R[,4] }
asset.rank <- c(2, 3, 1, 4)
```

### Meucci: Fully Flexible Views Framework
```{r eval=FALSE}
p <- rep(1 / nrow(R), nrow(R))
m.moments <- meucci.ranking(R, p, asset.rank)
```

### Almgren and Chriss: Portfolios from Sorts
```{r eval=FALSE}
ac.moments <- list()
ac.moments$mu <- ac.ranking(R, asset.rank)
# Sample estimate for second moment
ac.moments$sigma <- cov(R)
```

<!--
meucci.ranking does the entropy minimization and computes the first and second
moments given the market data and posterior probability.

EntropyProg does the entropy minimization and returns the posterior probabilities

meucci.moments computes the first and second moments given the market data and
posterior probability

ac.ranking computes the estimated centroid vector from a single complete sort
using the analytical approximation as described in R. Almgren and N. Chriss,
"Portfolios from Sorts". The centroid is estimated and then scaled such that it
is on a scale similar to the asset returns. By default, the centroid vector is
scaled according to the median of the asset mean returns.

-->

---

## Example Optimization

```{r eval=FALSE}
# Use moments output from meucci.ranking
opt.meucci <- optimize.portfolio(R, 
                                 init.portf, 
                                 optimize_method="random", 
                                 rp=rp, 
                                 trace=TRUE,
                                 momentargs=m.moments)

# Use first moment from ac.ranking. Note second moment is sample covariance
opt.ac <- optimize.portfolio(R, 
                             init.portf, 
                             optimize_method="random", 
                             rp=rp, 
                             trace=TRUE,
                             momentargs=ac.moments)
```


---

## Optimization Results
TODO: Insert image for optimal weights

---

## Custom Moment Function
```{r eval=FALSE, tidy=FALSE}
moment.ranking <- function(R, n=1, method=c("meucci", "ac")){
  method <- match.arg(method)
  tmpR <- apply(tail(R, n), 2, function(x) prod(1 + x) - 1)
  # Assume that the assets with the highest return will continue to outperform
  asset.rank <- order(tmpR)
  switch(method,
         meucci = {
           p <- rep(1 / nrow(R), nrow(R))
           moments <- meucci.ranking(R, p, asset.rank)
         },
         ac = {
           moments <- list()
           moments$mu <- ac.ranking(R, asset.rank)
           moments$sigma <- cov(R)
         })
  moments
}
```

---

## Optimization with Periodic Rebalancing

```{r eval=FALSE, tidy=FALSE}
opt.bt.meucci <- optimize.portfolio.rebalancing(R, init.portf, 
                                                optimize_method="random", 
                                                rebalance_on="quarters", 
                                                training_period=100,
                                                rp=rp,
                                                momentFUN="moment.ranking",
                                                n=3,
                                                method="meucci")

opt.bt.ac <- optimize.portfolio.rebalancing(R, init.portf, 
                                            optimize_method="random", 
                                            rebalance_on="quarters", 
                                            training_period=100,
                                            rp=rp,
                                            momentFUN="moment.ranking",
                                            n=3,
                                            method="ac")
```

---

## Optimization Results
TODO: Insert performance summary chart and table of annualized returns


---

## Conclusion

* Introduced the goals and summary of PortfolioAnalytics
* Demonstrated the flexibility through examples
* Plans for continued development
    * Interface to $parma$
    * Additional solvers
    * "Gallery" of examples

#### Acknowledgements
Many thanks to...

* Google: funding Google Summer of Code (GSoC) for 2014 and 2015
* UW CF&RM Program: continued work on PortfolioAnalytics
* GSoC Mentors: Brian Peterson, Peter Carl, Doug Martin, and Guy Yollin
* R/Finance Committee

<!---
- One of the best things about GSoC is the opportunity to work and interact with the mentors.
- Thank the GSoC mentors for offering help and guidance during the GSoC project and after as I continued to work on the PortfolioAnalytics package.
- R/Finance Committee for the conference and the opportunity to talk about PortfolioAnalytics.
- Google for funding the Google Summer of Code for PortfolioAnalytics and many other proposals for R

Thank everyone for attending
I hope they learned something and are motivated to use PortfolioAnalytics
-->

---

## PortfolioAnalytics Links
PortfolioAnalytics on CRAN

* [PortfolioAnalytics](http://cran.at.r-project.org/web/packages/PortfolioAnalytics/index.html)

PortfolioAnalytics development on R-Forge in the ReturnAnalytics project

* [PortfolioAnalytics](https://r-forge.r-project.org/projects/returnanalytics/)

Source code for the slides

* https://github.com/rossb34/PortfolioAnalyticsPresentation2015

and view it here

* http://rossb34.github.io/PortfolioAnalyticsPresentation2015/

---

## Any Questions?

---

## References and Useful Links

* [ROI](http://cran.r-project.org/web/packages/ROI/index.html)
* [DEoptim](http://cran.r-project.org/web/packages/DEoptim/index.html)
* [pso](http://cran.r-project.org/web/packages/pso/index.html)
* [GenSA](http://cran.r-project.org/web/packages/GenSA/index.html)
* [PerformanceAnalytics](http://cran.r-project.org/web/packages/PerformanceAnalytics/index.html)
* [Patrick Burns Random Portfolios](http://www.burns-stat.com/pages/Finance/randport_practice_theory_annotated.pdf)
* [W.T. Shaw Random Portfolios](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1856476)
* Martellini paper
* Boudt paper
* [Shiny App](http://spark.rstudio.com/rossbennett3/PortfolioOptimization/)
